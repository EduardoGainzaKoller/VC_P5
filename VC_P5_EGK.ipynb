{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "196aa37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from mtcnn import MTCNN\n",
    "from deepface import DeepFace\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65bca04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee05d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset_emociones\" \n",
    "train_dir = os.path.join(dataset_path, \"train\")\n",
    "val_dir = os.path.join(dataset_path, \"validation\")\n",
    "test_dir = os.path.join(dataset_path, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a3ffcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41367 images belonging to 5 classes.\n",
      "Found 11817 images belonging to 5 classes.\n",
      "Found 5915 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),  \n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2ff109c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m645\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">355,589</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m355,589\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">355,589</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m355,589\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# Modelo\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06a1608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97080564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 358ms/step - accuracy: 0.3777 - loss: 1.4593 - val_accuracy: 0.4944 - val_loss: 1.2257\n",
      "Epoch 2/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - accuracy: 0.4953 - loss: 1.2302 - val_accuracy: 0.5639 - val_loss: 1.0731\n",
      "Epoch 3/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 66ms/step - accuracy: 0.5395 - loss: 1.1408 - val_accuracy: 0.5886 - val_loss: 1.0104\n",
      "Epoch 4/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 66ms/step - accuracy: 0.5612 - loss: 1.0949 - val_accuracy: 0.5936 - val_loss: 1.0120\n",
      "Epoch 5/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 66ms/step - accuracy: 0.5744 - loss: 1.0621 - val_accuracy: 0.6063 - val_loss: 0.9852\n",
      "Epoch 6/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - accuracy: 0.5874 - loss: 1.0289 - val_accuracy: 0.6302 - val_loss: 0.9232\n",
      "Epoch 7/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 68ms/step - accuracy: 0.5950 - loss: 1.0187 - val_accuracy: 0.6286 - val_loss: 0.9262\n",
      "Epoch 8/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.5998 - loss: 0.9993 - val_accuracy: 0.6411 - val_loss: 0.8963\n",
      "Epoch 9/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6080 - loss: 0.9822 - val_accuracy: 0.6348 - val_loss: 0.9108\n",
      "Epoch 10/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6125 - loss: 0.9749 - val_accuracy: 0.6436 - val_loss: 0.8944\n",
      "Epoch 11/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - accuracy: 0.6153 - loss: 0.9685 - val_accuracy: 0.6433 - val_loss: 0.8826\n",
      "Epoch 12/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6195 - loss: 0.9593 - val_accuracy: 0.6375 - val_loss: 0.8999\n",
      "Epoch 13/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - accuracy: 0.6250 - loss: 0.9453 - val_accuracy: 0.6475 - val_loss: 0.8780\n",
      "Epoch 14/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6255 - loss: 0.9439 - val_accuracy: 0.6418 - val_loss: 0.8890\n",
      "Epoch 15/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6268 - loss: 0.9346 - val_accuracy: 0.6638 - val_loss: 0.8401\n",
      "Epoch 16/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - accuracy: 0.6295 - loss: 0.9331 - val_accuracy: 0.6552 - val_loss: 0.8562\n",
      "Epoch 17/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - accuracy: 0.6330 - loss: 0.9259 - val_accuracy: 0.6644 - val_loss: 0.8408\n",
      "Epoch 18/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6358 - loss: 0.9194 - val_accuracy: 0.6658 - val_loss: 0.8397\n",
      "Epoch 19/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 69ms/step - accuracy: 0.6391 - loss: 0.9132 - val_accuracy: 0.6631 - val_loss: 0.8511\n",
      "Epoch 20/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 76ms/step - accuracy: 0.6391 - loss: 0.9115 - val_accuracy: 0.6648 - val_loss: 0.8379\n",
      "Epoch 21/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 75ms/step - accuracy: 0.6416 - loss: 0.9078 - val_accuracy: 0.6682 - val_loss: 0.8266\n",
      "Epoch 22/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6416 - loss: 0.9023 - val_accuracy: 0.6644 - val_loss: 0.8493\n",
      "Epoch 23/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 68ms/step - accuracy: 0.6463 - loss: 0.8978 - val_accuracy: 0.6706 - val_loss: 0.8251\n",
      "Epoch 24/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - accuracy: 0.6465 - loss: 0.8948 - val_accuracy: 0.6653 - val_loss: 0.8369\n",
      "Epoch 25/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 68ms/step - accuracy: 0.6416 - loss: 0.8945 - val_accuracy: 0.6719 - val_loss: 0.8199\n",
      "Epoch 26/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6479 - loss: 0.8880 - val_accuracy: 0.6704 - val_loss: 0.8302\n",
      "Epoch 27/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6498 - loss: 0.8815 - val_accuracy: 0.6739 - val_loss: 0.8270\n",
      "Epoch 28/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6518 - loss: 0.8809 - val_accuracy: 0.6816 - val_loss: 0.7997\n",
      "Epoch 29/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6511 - loss: 0.8816 - val_accuracy: 0.6720 - val_loss: 0.8124\n",
      "Epoch 30/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - accuracy: 0.6511 - loss: 0.8783 - val_accuracy: 0.6800 - val_loss: 0.8005\n",
      "Epoch 31/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 69ms/step - accuracy: 0.6544 - loss: 0.8713 - val_accuracy: 0.6788 - val_loss: 0.8085\n",
      "Epoch 32/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6531 - loss: 0.8730 - val_accuracy: 0.6791 - val_loss: 0.8119\n",
      "Epoch 33/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6589 - loss: 0.8678 - val_accuracy: 0.6805 - val_loss: 0.8076\n",
      "Epoch 34/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 66ms/step - accuracy: 0.6584 - loss: 0.8665 - val_accuracy: 0.6819 - val_loss: 0.8046\n",
      "Epoch 35/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6588 - loss: 0.8665 - val_accuracy: 0.6772 - val_loss: 0.8131\n",
      "Epoch 36/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6639 - loss: 0.8566 - val_accuracy: 0.6758 - val_loss: 0.8215\n",
      "Epoch 37/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - accuracy: 0.6599 - loss: 0.8565 - val_accuracy: 0.6777 - val_loss: 0.8041\n",
      "Epoch 38/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - accuracy: 0.6611 - loss: 0.8537 - val_accuracy: 0.6805 - val_loss: 0.7972\n",
      "Epoch 39/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6600 - loss: 0.8565 - val_accuracy: 0.6881 - val_loss: 0.7895\n",
      "Epoch 40/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 66ms/step - accuracy: 0.6650 - loss: 0.8517 - val_accuracy: 0.6767 - val_loss: 0.8087\n",
      "Epoch 41/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6637 - loss: 0.8527 - val_accuracy: 0.6734 - val_loss: 0.8193\n",
      "Epoch 42/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6674 - loss: 0.8457 - val_accuracy: 0.6860 - val_loss: 0.7934\n",
      "Epoch 43/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6647 - loss: 0.8525 - val_accuracy: 0.6853 - val_loss: 0.7844\n",
      "Epoch 44/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6657 - loss: 0.8501 - val_accuracy: 0.6838 - val_loss: 0.7905\n",
      "Epoch 45/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6664 - loss: 0.8465 - val_accuracy: 0.6910 - val_loss: 0.7801\n",
      "Epoch 46/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6697 - loss: 0.8387 - val_accuracy: 0.6951 - val_loss: 0.7818\n",
      "Epoch 47/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 66ms/step - accuracy: 0.6700 - loss: 0.8407 - val_accuracy: 0.6849 - val_loss: 0.7862\n",
      "Epoch 48/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6676 - loss: 0.8391 - val_accuracy: 0.6855 - val_loss: 0.7838\n",
      "Epoch 49/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.6695 - loss: 0.8358 - val_accuracy: 0.6890 - val_loss: 0.7833\n",
      "Epoch 50/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6697 - loss: 0.8374 - val_accuracy: 0.6871 - val_loss: 0.7871\n",
      "Epoch 51/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6684 - loss: 0.8340 - val_accuracy: 0.6888 - val_loss: 0.7871\n",
      "Epoch 52/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - accuracy: 0.6674 - loss: 0.8384 - val_accuracy: 0.6938 - val_loss: 0.7734\n",
      "Epoch 53/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6710 - loss: 0.8391 - val_accuracy: 0.6857 - val_loss: 0.7938\n",
      "Epoch 54/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6682 - loss: 0.8379 - val_accuracy: 0.6892 - val_loss: 0.7865\n",
      "Epoch 55/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 67ms/step - accuracy: 0.6715 - loss: 0.8319 - val_accuracy: 0.6893 - val_loss: 0.7789\n",
      "Epoch 56/150\n",
      "\u001b[1m1293/1293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 70ms/step - accuracy: 0.6722 - loss: 0.8312 - val_accuracy: 0.6951 - val_loss: 0.7833\n",
      "Epoch 56: early stopping\n",
      "Restoring model weights from the end of the best epoch: 46.\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=[es]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41ed43a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"modelo_emociones.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "daf0dd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model_emotions = load_model(\"modelo_emociones.h5\")\n",
    "\n",
    "\n",
    "# Lista de clases del modelo de emociones\n",
    "emotion_classes = ['Angry','Fear','Happy','Sad','Surprise']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "075fda23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------- configuración -----------------\n",
    "ASSETS_DIR = \"assets\"   \n",
    "MODEL_PATH = \"modelo_emociones.h5\"\n",
    "\n",
    "\n",
    "def load_png(path):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  \n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    if img.shape[2] == 3:\n",
    "        b,g,r = cv2.split(img)\n",
    "        alpha = np.ones(b.shape, dtype=b.dtype) * 255\n",
    "        img = cv2.merge([b,g,r,alpha])\n",
    "    return img\n",
    "\n",
    "def overlay_image_alpha(img, img_overlay, x, y, overlay_size=None):\n",
    "    if img_overlay is None:\n",
    "        return img\n",
    "    overlay = img_overlay.copy()\n",
    "    if overlay_size is not None:\n",
    "        overlay = cv2.resize(overlay, overlay_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    b,g,r,a = cv2.split(overlay)\n",
    "    overlay_rgb = cv2.merge((b,g,r))\n",
    "    mask = cv2.merge((a,a,a))\n",
    "\n",
    "    h, w = overlay_rgb.shape[:2]\n",
    "    if x < 0:\n",
    "        overlay_rgb = overlay_rgb[:, -x:]\n",
    "        mask = mask[:, -x:]\n",
    "        w += x\n",
    "        x = 0\n",
    "    if y < 0:\n",
    "        overlay_rgb = overlay_rgb[-y:, :]\n",
    "        mask = mask[-y:, :]\n",
    "        h += y\n",
    "        y = 0\n",
    "\n",
    "    \n",
    "    if y + h > img.shape[0]:\n",
    "        h = img.shape[0] - y\n",
    "        overlay_rgb = overlay_rgb[:h]\n",
    "        mask = mask[:h]\n",
    "    if x + w > img.shape[1]:\n",
    "        w = img.shape[1] - x\n",
    "        overlay_rgb = overlay_rgb[:, :w]\n",
    "        mask = mask[:, :w]\n",
    "\n",
    "    if h <= 0 or w <= 0:\n",
    "        return img\n",
    "\n",
    "    roi = img[y:y+h, x:x+w]\n",
    "    \n",
    "    alpha = mask.astype(float) / 255.0\n",
    "    overlay_rgb = overlay_rgb.astype(float)\n",
    "    roi = roi.astype(float)\n",
    "\n",
    "    blended = cv2.multiply(alpha, overlay_rgb) + cv2.multiply(1 - alpha, roi)\n",
    "    img[y:y+h, x:x+w] = blended.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "# ----------------- cargar assets -----------------\n",
    "aura_png = load_png(os.path.join(ASSETS_DIR, \"aura.png\"))\n",
    "halo_png = load_png(os.path.join(ASSETS_DIR, \"halo.png\"))\n",
    "eye_glow_png = load_png(os.path.join(ASSETS_DIR, \"eyes.png\"))\n",
    "tear_png = load_png(os.path.join(ASSETS_DIR, \"tears.png\"))\n",
    "\n",
    "# ----------------- cargar modelo y detector -----------------\n",
    "detector = MTCNN()\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, x, y, vx, vy, life, color, size):\n",
    "        self.x = x; self.y = y\n",
    "        self.vx = vx; self.vy = vy\n",
    "        self.life = life\n",
    "        self.color = color\n",
    "        self.size = size\n",
    "\n",
    "    def update(self, dt=1.0):\n",
    "        self.x += self.vx * dt\n",
    "        self.y += self.vy * dt\n",
    "        self.vy += 0.3 * dt  \n",
    "        self.life -= dt\n",
    "\n",
    "    def is_alive(self):\n",
    "        return self.life > 0\n",
    "\n",
    "particles = []\n",
    "\n",
    "\n",
    "def update_and_draw_particles(frame):\n",
    "    for p in particles[:]:\n",
    "        p.update()\n",
    "        if not p.is_alive():\n",
    "            particles.remove(p)\n",
    "            continue\n",
    "        cv2.circle(frame, (int(p.x), int(p.y)), p.size, p.color, -1)\n",
    "\n",
    "\n",
    "def preprocess_face_for_emotion(face_bgr):\n",
    "    gray = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, (48,48))\n",
    "    gray = gray.astype(\"float32\") / 255.0\n",
    "    gray = np.expand_dims(gray, axis=(0,-1)) \n",
    "    return gray\n",
    "\n",
    "\n",
    "def effect_happy(frame, x, y, w, h, keypoints):\n",
    "    \n",
    "    if halo_png is not None:\n",
    "        halo_w = int(w * 1.1)\n",
    "        halo_h = int(halo_w * halo_png.shape[0] / halo_png.shape[1])\n",
    "        hx = x + w//2 - halo_w//2\n",
    "        hy = y - int(h * 0.55)\n",
    "        frame = overlay_image_alpha(frame, halo_png, hx, hy, (halo_w, halo_h))\n",
    "    \n",
    "    for _ in range(6):\n",
    "        cx = x + random.randint(0, w)\n",
    "        cy = y + random.randint(-h//2, h)\n",
    "        cv2.circle(frame, (cx, cy), random.randint(2,5), (random.randint(200,255), random.randint(100,255), random.randint(50,255)), -1)\n",
    "    \n",
    "    overlay = frame.copy()\n",
    "    cv2.rectangle(overlay, (x, y), (x+w, y+h), (255, 240, 200), -1)\n",
    "    cv2.addWeighted(overlay, 0.03, frame, 0.97, 0, frame)\n",
    "    return frame\n",
    "\n",
    "def effect_sad(frame, x, y, w, h, keypoints):\n",
    "   \n",
    "    left_eye = keypoints.get('left_eye')\n",
    "    right_eye = keypoints.get('right_eye')\n",
    "\n",
    "    if tear_png is not None and left_eye and right_eye:\n",
    "\n",
    "        eye_center_x = (left_eye[0] + right_eye[0]) // 2\n",
    "\n",
    "        tears_width = int(w * 1.3) \n",
    "        aspect = tear_png.shape[0] / tear_png.shape[1]\n",
    "        tears_height = int(tears_width * aspect)\n",
    "\n",
    "        top_left_x = eye_center_x - tears_width // 2\n",
    "        top_left_y = int((left_eye[1] + right_eye[1]) / 2 + h * 1)\n",
    "\n",
    "        frame = overlay_image_alpha(frame, tear_png, top_left_x, top_left_y, (tears_width, tears_height))\n",
    "\n",
    "    else:\n",
    "        ex = x + int(w * 0.3)\n",
    "        ey = y + int(h * 0.55)\n",
    "        for i in range(3):\n",
    "            cv2.circle(frame, (ex, ey + i*12), 5+i, (220,180,255), -1)\n",
    "        ex2 = x + int(w * 0.7)\n",
    "        ey2 = y + int(h * 0.55)\n",
    "        for i in range(3):\n",
    "            cv2.circle(frame, (ex2, ey2 + i*12), 5+i, (220,180,255), -1)\n",
    "\n",
    "    \n",
    "    overlay = frame.copy()\n",
    "    cv2.rectangle(overlay, (0,0), (frame.shape[1], frame.shape[0]), (20,20,40), -1)\n",
    "    cv2.addWeighted(overlay, 0.18, frame, 0.82, 0, frame)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def effect_surprise(frame, x, y, w, h, keypoints):\n",
    "    overlay = frame.copy()\n",
    "    cv2.rectangle(overlay, (x, y), (x+w, y+h), (255,255,255), -1)\n",
    "    cv2.addWeighted(overlay, 0.18, frame, 0.82, 0, frame)\n",
    "    for i in range(8):\n",
    "        sx = x + random.randint(-w//4, w + w//4)\n",
    "        sy = y + random.randint(-h//4, h + h//4)\n",
    "        cv2.circle(frame, (sx, sy), random.randint(1,3), (255,255,255), -1)\n",
    "    return frame\n",
    "\n",
    "def effect_neutral(frame, x, y, w, h, keypoints):\n",
    "    face_roi = frame[y:y+h, x:x+w]\n",
    "    if face_roi.size != 0:\n",
    "        smooth = cv2.bilateralFilter(face_roi, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "        frame[y:y+h, x:x+w] = cv2.addWeighted(face_roi, 0.6, smooth, 0.4, 0)\n",
    "    return frame\n",
    "\n",
    "def effect_angry(frame, x, y, w, h, keypoints):\n",
    "    \n",
    "    if aura_png is not None:\n",
    "        aura_w = int(w * 2.0)\n",
    "        aura_h = int(aura_w * aura_png.shape[0] / aura_png.shape[1])\n",
    "        ax = x + w//2 - aura_w//2\n",
    "        ay = y - int(h * 0.65)\n",
    "        frame = overlay_image_alpha(frame, aura_png, ax, ay, (aura_w, aura_h))\n",
    "    else:\n",
    "        \n",
    "        overlay = frame.copy()\n",
    "        cv2.circle(overlay, (x + w//2, y + h//2 - int(h*0.2)), int(w*0.9), (0,200,255), -1)\n",
    "        cv2.addWeighted(overlay, 0.15, frame, 0.85, 0, frame)\n",
    "\n",
    "\n",
    "    return frame\n",
    "\n",
    "# ----------------- bucle principal webcam -----------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "fps_last = time.time()\n",
    "frame_count = 0\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "output_fps = 30\n",
    "out = cv2.VideoWriter(\"demo_emociones.mp4\", fourcc, output_fps,\n",
    "                      (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    detections = detector.detect_faces(frame)\n",
    "\n",
    "    for det in detections:\n",
    "        x, y, w, h = det['box']\n",
    "        x, y = max(0, x), max(0, y)\n",
    "        keypoints = det.get('keypoints', {})\n",
    "\n",
    "        \n",
    "        CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "        try:\n",
    "            face = frame[y:y+h, x:x+w]\n",
    "            if face.size == 0:\n",
    "                continue\n",
    "\n",
    "            inp = preprocess_face_for_emotion(face)\n",
    "            pred = model.predict(inp)[0]       \n",
    "            max_index = np.argmax(pred)\n",
    "            max_conf = pred[max_index]\n",
    "\n",
    "            if max_conf >= CONFIDENCE_THRESHOLD:\n",
    "                emo = emotion_classes[max_index]\n",
    "            else:\n",
    "                emo = \"Neutral\"\n",
    "\n",
    "        except Exception as e:\n",
    "            emo = \"Neutral\"\n",
    "\n",
    "        \n",
    "        if emo == \"Happy\":\n",
    "            frame = effect_happy(frame, x, y, w, h, keypoints)\n",
    "        elif emo == \"Sad\":\n",
    "            frame = effect_sad(frame, x, y, w, h, keypoints)\n",
    "        elif emo == \"Suprise\":\n",
    "            frame = effect_surprise(frame, x, y, w, h, keypoints)\n",
    "        elif emo == \"neutral\":\n",
    "            frame = effect_neutral(frame, x, y, w, h, keypoints)\n",
    "        elif emo == \"Angry\":\n",
    "            frame = effect_angry(frame, x, y, w, h, keypoints)\n",
    "        else:\n",
    "            pass  \n",
    "\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255,255,255), 1)\n",
    "        cv2.putText(frame, emo.upper(), (x, y-10), cv2.FONT_HERSHEY_DUPLEX, 0.7, (255,255,255), 2)\n",
    "\n",
    "\n",
    "    \n",
    "    update_and_draw_particles(frame)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "    frame_count += 1\n",
    "    if time.time() - fps_last >= 1.0:\n",
    "        fps = frame_count / (time.time() - fps_last)\n",
    "        fps_last = time.time()\n",
    "        frame_count = 0\n",
    "    cv2.putText(frame, f\"Detector de emociones\", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200,200,200), 1)\n",
    "\n",
    "    cv2.imshow(\"Detector de emociones\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release() \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcdc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2de6c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controles:\n",
      "'2' - Filtro 2: Solo ojos y boca agrandados\n",
      "'3' - Filtro 3: Ojos y boca grandes sobre cara normal\n",
      "'q' - Salir\n"
     ]
    }
   ],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Índices de landmarks para cada parte facial\n",
    "LEFT_EYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]\n",
    "RIGHT_EYE = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]\n",
    "MOUTH_OUTER = [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 409, 270, 269, 267, 0, 37, 39, 40, 185]\n",
    "NOSE = [168, 6, 197, 195, 5, 4, 1, 19, 94, 2]\n",
    "\n",
    "\n",
    "def create_mask_from_landmarks(landmarks, indices, width, height, padding=5):\n",
    "    points = np.array([(int(landmarks[i].x * width),\n",
    "                        int(landmarks[i].y * height)) for i in indices])\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, [points], 255)\n",
    "    kernel = np.ones((padding, padding), np.uint8)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_bounding_box(landmarks, indices, width, height, padding=20):\n",
    "    points = [(int(landmarks[i].x * width),\n",
    "               int(landmarks[i].y * height)) for i in indices]\n",
    "    x_coords = [p[0] for p in points]\n",
    "    y_coords = [p[1] for p in points]\n",
    "    x_min = max(0, min(x_coords) - padding)\n",
    "    x_max = min(width, max(x_coords) + padding)\n",
    "    y_min = max(0, min(y_coords) - padding)\n",
    "    y_max = min(height, max(y_coords) + padding)\n",
    "    center_x = (x_min + x_max) // 2\n",
    "    center_y = (y_min + y_max) // 2\n",
    "    return (center_x, center_y), (x_min, y_min, x_max, y_max)\n",
    "\n",
    "\n",
    "def extract_face_part_with_mask(frame, landmarks, indices, padding=20):\n",
    "    height, width = frame.shape[:2]\n",
    "    mask = create_mask_from_landmarks(landmarks, indices, width, height, padding=3)\n",
    "    center, (x_min, y_min, x_max, y_max) = get_bounding_box(landmarks, indices, width, height, padding)\n",
    "    part_img = frame[y_min:y_max, x_min:x_max].copy()\n",
    "    mask_crop = mask[y_min:y_max, x_min:x_max]\n",
    "    result = np.zeros_like(part_img)\n",
    "    result[mask_crop > 0] = part_img[mask_crop > 0]\n",
    "    return result, center\n",
    "\n",
    "\n",
    "def place_part_on_canvas(canvas, part_img, target_pos, angle=0, scale=1.0):\n",
    "    if part_img is None or part_img.size == 0:\n",
    "        return\n",
    "    h, w = part_img.shape[:2]\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "    if new_w <= 0 or new_h <= 0:\n",
    "        return\n",
    "    scaled = cv2.resize(part_img, (new_w, new_h))\n",
    "    center = (new_w // 2, new_h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(scaled, M, (new_w, new_h),\n",
    "                             borderMode=cv2.BORDER_CONSTANT,\n",
    "                             borderValue=(0, 0, 0))\n",
    "    x = target_pos[0] - new_w // 2\n",
    "    y = target_pos[1] - new_h // 2\n",
    "    x = max(0, min(x, canvas.shape[1] - new_w))\n",
    "    y = max(0, min(y, canvas.shape[0] - new_h))\n",
    "    gray = cv2.cvtColor(rotated, cv2.COLOR_BGR2GRAY)\n",
    "    _, mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "    mask_inv = cv2.bitwise_not(mask)\n",
    "    roi = canvas[y:y+new_h, x:x+new_w]\n",
    "    if roi.shape[0] == new_h and roi.shape[1] == new_w:\n",
    "        roi_bg = cv2.bitwise_and(roi, roi, mask=mask_inv)\n",
    "        part_fg = cv2.bitwise_and(rotated, rotated, mask=mask)\n",
    "        canvas[y:y+new_h, x:x+new_w] = cv2.add(roi_bg, part_fg)\n",
    "\n",
    "\n",
    "# --- Filtros ---\n",
    "def create_face_collage_filter2(frame, landmarks):\n",
    "    height, width = frame.shape[:2]\n",
    "    canvas = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    left_eye_img, left_eye_center = extract_face_part_with_mask(frame, landmarks, LEFT_EYE, padding=15)\n",
    "    right_eye_img, right_eye_center = extract_face_part_with_mask(frame, landmarks, RIGHT_EYE, padding=15)\n",
    "    mouth_img, mouth_center = extract_face_part_with_mask(frame, landmarks, MOUTH_OUTER, padding=20)\n",
    "    scale_factor = 1.8\n",
    "    place_part_on_canvas(canvas, left_eye_img, left_eye_center, scale=scale_factor)\n",
    "    place_part_on_canvas(canvas, right_eye_img, right_eye_center, scale=scale_factor)\n",
    "    place_part_on_canvas(canvas, mouth_img, mouth_center, scale=scale_factor)\n",
    "    return canvas\n",
    "\n",
    "\n",
    "def create_face_collage_filter3(frame, landmarks):\n",
    "    canvas = frame.copy()\n",
    "    left_eye_img, left_eye_center = extract_face_part_with_mask(frame, landmarks, LEFT_EYE, padding=15)\n",
    "    right_eye_img, right_eye_center = extract_face_part_with_mask(frame, landmarks, RIGHT_EYE, padding=15)\n",
    "    mouth_img, mouth_center = extract_face_part_with_mask(frame, landmarks, MOUTH_OUTER, padding=20)\n",
    "    scale_factor = 1.8\n",
    "    place_part_on_canvas(canvas, left_eye_img, left_eye_center, scale=scale_factor)\n",
    "    place_part_on_canvas(canvas, right_eye_img, right_eye_center, scale=scale_factor)\n",
    "    place_part_on_canvas(canvas, mouth_img, mouth_center, scale=scale_factor)\n",
    "    return canvas\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# CAPTURA DE VIDEO\n",
    "# ------------------------------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "current_filter = 2\n",
    "\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "fps = 30\n",
    "width = int(cap.get(3))\n",
    "height = int(cap.get(4))\n",
    "out = cv2.VideoWriter(\"filtro_demo.mp4\", fourcc, fps, (width, height))\n",
    "\n",
    "\n",
    "print(\"Controles:\")\n",
    "print(\"'2' - Filtro 2: Solo ojos y boca agrandados\")\n",
    "print(\"'3' - Filtro 3: Ojos y boca grandes sobre cara normal\")\n",
    "print(\"'q' - Salir\")\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "\n",
    "        if current_filter == 2:\n",
    "            collage = create_face_collage_filter2(frame, landmarks)\n",
    "            filter_name = \"Filtro 2: Ojos + Boca (Grande)\"\n",
    "        else:\n",
    "            collage = create_face_collage_filter3(frame, landmarks)\n",
    "            filter_name = \"Filtro 3: Ojos + Boca (Sobre cara)\"\n",
    "\n",
    "        cv2.putText(collage, filter_name, (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Face Collage', collage)\n",
    "\n",
    "        \n",
    "        out.write(collage)\n",
    "\n",
    "    else:\n",
    "        cv2.imshow('Face Collage', frame)\n",
    "        out.write(frame)   \n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('2'):\n",
    "        current_filter = 2\n",
    "    elif key == ord('3'):\n",
    "        current_filter = 3\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# LIBERAR RECURSOS\n",
    "# ------------------------------------------------\n",
    "cap.release()\n",
    "out.release()   \n",
    "cv2.destroyAllWindows()\n",
    "face_mesh.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5_mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
